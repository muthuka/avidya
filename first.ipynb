{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the implementation of Neural Networks from scratch in detail\n",
    "\n",
    "Now that you have gone through a basic implementation of numpy from scratch in both Python and R, we will dive deep into understanding each code block and try to apply the same code on a different dataset. We will also visualize how our model is working, by “debugging” it step by step using the interactive environment of a jupyter notebook and using basic data science tools such as numpy and matplotlib. So let’s get started!\n",
    "\n",
    "The first thing we will do is to import the libraries mentioned before, namely numpy and matplotlib. Also, as we will be working with the jupyter notebook IDE, we will set inline plotting of graphs using the magic function %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install -q matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of numpy: 1.26.2\n",
      "Version of matplotlib: 3.8.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# version of numpy library\n",
    "print(\"Version of numpy:\", np.__version__)\n",
    "\n",
    "# version of matplotlib library\n",
    "print(\"Version of matplotlib:\", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set the random seed parameter to a specific number (let’s say 42 (as we already know that is the answer to everything!)) so that the code we run gives us the same output every time we run (hopefully!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next step is to create our input. Firstly, let’s take a dummy dataset, where only the first column is a useful column, whereas the rest may or may not be useful and can be a potential noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " [[1 0 0 0]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]]\n",
      "\n",
      "Shape of Input: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "# creating the input array\n",
    "X = np.array([[1, 0, 0, 0], [1, 0, 1, 1], [0, 1, 0, 1]])\n",
    "\n",
    "print(\"Input:\\n\", X)\n",
    "\n",
    "# shape of input array\n",
    "print(\"\\nShape of Input:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as you might remember, we have to take the transpose of input so that we can train our network. Let’s do that quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input in matrix form:\n",
      " [[1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 1]]\n",
      "\n",
      "Shape of Input Matrix: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "# converting the input in matrix form\n",
    "X = X.T\n",
    "print(\"Input in matrix form:\\n\", X)\n",
    "\n",
    "# shape of input matrix\n",
    "print(\"\\nShape of Input Matrix:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s create our output array and transpose that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Output:\n",
      " [[1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Output in matrix form:\n",
      " [[1 1 0]]\n",
      "\n",
      "Shape of Output: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# creating the output array\n",
    "y = np.array([[1], [1], [0]])\n",
    "\n",
    "print(\"Actual Output:\\n\", y)\n",
    "\n",
    "# output in matrix form\n",
    "y = y.T\n",
    "\n",
    "print(\"\\nOutput in matrix form:\\n\", y)\n",
    "\n",
    "# shape of input array\n",
    "print(\"\\nShape of Output:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our input and output data is ready, let’s define our neural network. We will define a very simple architecture, having one hidden layer with just three neurons\n",
    "\n",
    "![Architecture](images/model_architecture.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer_neurons = X.shape[0]  # number of features in data set\n",
    "hiddenLayer_neurons = 3  # number of hidden layers neurons\n",
    "outputLayer_neurons = 1  # number of neurons at output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will initialize the weights for each neuron in the network. The weights we create have values ranging from 0 to 1, which we initialize randomly at the start.\n",
    "\n",
    "For simplicity, we will not include bias in the calculations, but you can check the simple implementation we did before to see how it works for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing weight\n",
    "# Shape of weights_input_hidden should number of neurons at input layer * number of neurons at hidden layer\n",
    "weights_input_hidden = np.random.uniform(size=(inputLayer_neurons, hiddenLayer_neurons))\n",
    "\n",
    "# Shape of weights_hidden_output should number of neurons at hidden layer * number of neurons at output layer\n",
    "weights_hidden_output = np.random.uniform(\n",
    "    size=(hiddenLayer_neurons, outputLayer_neurons)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s print the shapes of these numpy arrays for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (3, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of weight matrix\n",
    "weights_input_hidden.shape, weights_hidden_output.shape# We are using sigmoid as an activation function so defining the sigmoid function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we will define our activation function as sigmoid, which we will use in both the hidden layer and output layer of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using sigmoid as an activation function so defining the sigmoid function here\n",
    "\n",
    "# defining the Sigmoid Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we will implement our forward pass, first to get the hidden layer activations and then for the output layer. Our forward pass would look something like this\n",
    "![WRT](images/error_wrt_who.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer activations\n",
    "\n",
    "hiddenLayer_linearTransform = np.dot(weights_input_hidden.T, X)\n",
    "hiddenLayer_activations = sigmoid(hiddenLayer_linearTransform)\n",
    "\n",
    "# calculating the output\n",
    "outputLayer_linearTransform = np.dot(weights_hidden_output.T, hiddenLayer_activations)\n",
    "output = sigmoid(outputLayer_linearTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see what our untrained model gives as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68334694, 0.72697078, 0.71257368]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed our forward propagation step and got the error. Now let’s do a backward propagation to calculate the error with respect to each weight of the neuron and then update these weights using simple gradient descent.\n",
    "\n",
    "Firstly we will calculate the error with respect to weights between the hidden and output layers. Essentially, we will do an operation such as this\n",
    "![third](images/third.png)\n",
    "\n",
    "\n",
    "where to calculate this, the following would be our intermediate steps using the chain rule\n",
    "\n",
    "* Rate of change of error w.r.t output\n",
    "* Rate of change of output w.r.t Z2\n",
    "* Rate of change of Z2 w.r.t weights between hidden and output layer\n",
    "\n",
    "Let’s perform the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate of change of error w.r.t. output\n",
    "error_wrt_output = -(y - output)\n",
    "\n",
    "# rate of change of output w.r.t. Z2\n",
    "output_wrt_outputLayer_LinearTransform = np.multiply(output, (1 - output))\n",
    "\n",
    "# rate of change of Z2 w.r.t. weights between hidden and output layer\n",
    "outputLayer_LinearTransform_wrt_weights_hidden_output = hiddenLayer_activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s check the shapes of the intermediate operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (1, 3), (3, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the shapes of partial derivatives\n",
    "error_wrt_output.shape, output_wrt_outputLayer_LinearTransform.shape, outputLayer_LinearTransform_wrt_weights_hidden_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is an output shape like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of weights of output layer\n",
    "weights_hidden_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we saw before, we can define this operation formally using this equation\n",
    "\n",
    "![forth](images/fourth.png)\n",
    "\n",
    "Let’s perform the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rate of change of error w.r.t weight between hidden and output layer\n",
    "error_wrt_weights_hidden_output = np.dot(\n",
    "    outputLayer_LinearTransform_wrt_weights_hidden_output,\n",
    "    (error_wrt_output * output_wrt_outputLayer_LinearTransform).T,\n",
    ")\n",
    "\n",
    "error_wrt_weights_hidden_output.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
