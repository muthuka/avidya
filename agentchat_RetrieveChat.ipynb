{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "# Auto Generated Agent Chat: Using RetrieveChat for Retrieve Augmented Code Generation and Question Answering\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "RetrieveChat is a conversational system for retrieval-augmented code generation and question answering. In this notebook, we demonstrate how to utilize RetrieveChat to generate code and answer questions based on customized documentations that are not present in the LLM's training dataset. RetrieveChat uses the `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`, which is similar to the usage of `AssistantAgent` and `UserProxyAgent` in other notebooks (e.g., [Automated Task Solving with Code Generation, Execution & Debugging](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)). Essentially, `RetrieveAssistantAgent` and  `RetrieveUserProxyAgent` implement a different auto-reply mechanism corresponding to the RetrieveChat prompts.\n",
    "\n",
    "## Table of Contents\n",
    "We'll demonstrate six examples of using RetrieveChat for code generation and question answering:\n",
    "\n",
    "[Example 1: Generate code based off docstrings w/o human feedback](#example-1)\n",
    "\n",
    "[Example 2: Answer a question based off docstrings w/o human feedback](#example-2)\n",
    "\n",
    "[Example 3: Generate code based off docstrings w/ human feedback](#example-3)\n",
    "\n",
    "[Example 4: Answer a question based off docstrings w/ human feedback](#example-4)\n",
    "\n",
    "[Example 5: Solve comprehensive QA problems with RetrieveChat's unique feature `Update Context`](#example-5)\n",
    "\n",
    "[Example 6: Solve comprehensive QA problems with customized prompt and few-shot learning](#example-6)\n",
    "\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install the [retrievechat] option.\n",
    "```bash\n",
    "pip install \"pyautogen[retrievechat]~=0.2.0b5\" \"flaml[automl]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen~=0.2.0b5 (from pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached pyautogen-0.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting flaml[automl]\n",
      "  Using cached FLAML-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting diskcache (from pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting openai~=1.3 (from pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached openai-1.3.8-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting python-dotenv (from pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting termcolor (from pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tiktoken (from pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached tiktoken-0.5.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in ./.venv/lib/python3.11/site-packages (from flaml[automl]) (1.26.2)\n",
      "Requirement already satisfied: lightgbm>=2.3.1 in ./.venv/lib/python3.11/site-packages (from flaml[automl]) (4.1.0)\n",
      "Collecting xgboost>=0.90 (from flaml[automl])\n",
      "  Using cached xgboost-2.0.2-py3-none-macosx_12_0_arm64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./.venv/lib/python3.11/site-packages (from flaml[automl]) (1.11.4)\n",
      "Collecting pandas>=1.1.4 (from flaml[automl])\n",
      "  Using cached pandas-2.1.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in ./.venv/lib/python3.11/site-packages (from flaml[automl]) (1.3.2)\n",
      "Collecting chromadb (from pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached chromadb-0.4.18-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: ipython in ./.venv/lib/python3.11/site-packages (from pyautogen[retrievechat]~=0.2.0b5) (8.18.1)\n",
      "Collecting pypdf (from pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached pypdf-3.17.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting sentence-transformers (from pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting anyio<5,>=3.5.0 (from openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached anyio-4.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting sniffio (from openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in ./.venv/lib/python3.11/site-packages (from openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas>=1.1.4->flaml[automl]) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.4->flaml[automl])\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas>=1.1.4->flaml[automl])\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24->flaml[automl]) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24->flaml[automl]) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.28 in ./.venv/lib/python3.11/site-packages (from chromadb->pyautogen[retrievechat]~=0.2.0b5) (2.31.0)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached posthog-3.1.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached pulsar_client-3.3.0-cp311-cp311-macosx_10_15_universal2.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached onnxruntime-1.16.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_api-1.21.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_sdk-1.21.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.11/site-packages (from chromadb->pyautogen[retrievechat]~=0.2.0b5) (0.15.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Collecting overrides>=7.3.1 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached overrides-7.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-resources (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached grpcio-1.60.0-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached bcrypt-4.1.1-cp37-abi3-macosx_13_0_universal2.whl.metadata (9.2 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached kubernetes-28.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in ./.venv/lib/python3.11/site-packages (from chromadb->pyautogen[retrievechat]~=0.2.0b5) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached mmh3-4.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython->pyautogen[retrievechat]~=0.2.0b5) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython->pyautogen[retrievechat]~=0.2.0b5) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython->pyautogen[retrievechat]~=0.2.0b5) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython->pyautogen[retrievechat]~=0.2.0b5) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython->pyautogen[retrievechat]~=0.2.0b5) (2.17.2)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.11/site-packages (from ipython->pyautogen[retrievechat]~=0.2.0b5) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in ./.venv/lib/python3.11/site-packages (from ipython->pyautogen[retrievechat]~=0.2.0b5) (5.14.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython->pyautogen[retrievechat]~=0.2.0b5) (4.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (4.35.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (2.1.1)\n",
      "Collecting torchvision (from sentence-transformers->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached torchvision-0.16.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting nltk (from sentence-transformers->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece (from sentence-transformers->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached sentencepiece-0.1.99-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (0.19.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5) (2023.10.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5) (3.6)\n",
      "Collecting anyio<5,>=3.5.0 (from openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (2023.12.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (23.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython->pyautogen[retrievechat]~=0.2.0b5) (0.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5) (1.16.0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached google_auth-2.25.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[retrievechat]~=0.2.0b5) (1.12)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_proto-1.21.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_instrumentation-0.42b0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached opentelemetry_util_http-0.42b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[retrievechat]~=0.2.0b5) (68.2.2)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython->pyautogen[retrievechat]~=0.2.0b5) (0.7.0)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->pyautogen[retrievechat]~=0.2.0b5) (0.2.12)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1.9.0->openai~=1.3->pyautogen~=0.2.0b5->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached pydantic_core-2.14.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.28->chromadb->pyautogen[retrievechat]~=0.2.0b5) (3.3.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (3.1.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (0.4.1)\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer>=0.9.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached watchfiles-0.21.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack-data->ipython->pyautogen[retrievechat]~=0.2.0b5) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack-data->ipython->pyautogen[retrievechat]~=0.2.0b5) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack-data->ipython->pyautogen[retrievechat]~=0.2.0b5) (0.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from torchvision->sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (10.1.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers->pyautogen[retrievechat]~=0.2.0b5) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->pyautogen[retrievechat]~=0.2.0b5) (1.3.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[retrievechat]~=0.2.0b5)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached pyautogen-0.2.2-py3-none-any.whl (124 kB)\n",
      "Using cached openai-1.3.8-py3-none-any.whl (221 kB)\n",
      "Using cached pandas-2.1.4-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached xgboost-2.0.2-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "Using cached chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
      "Using cached chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl (198 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
      "Using cached pypdf-3.17.2-py3-none-any.whl (277 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tiktoken-0.5.2-cp311-cp311-macosx_11_0_arm64.whl (953 kB)\n",
      "Using cached bcrypt-4.1.1-cp37-abi3-macosx_13_0_universal2.whl (528 kB)\n",
      "Using cached fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached grpcio-1.60.0-cp311-cp311-macosx_10_10_universal2.whl (9.7 MB)\n",
      "Using cached httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Using cached kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached mmh3-4.0.1-cp311-cp311-macosx_11_0_arm64.whl (35 kB)\n",
      "Using cached onnxruntime-1.16.3-cp311-cp311-macosx_11_0_arm64.whl (6.2 MB)\n",
      "Using cached opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
      "Using cached opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
      "Using cached opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
      "Using cached opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
      "Using cached overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
      "Using cached pulsar_client-3.3.0-cp311-cp311-macosx_10_15_universal2.whl (10.9 MB)\n",
      "Using cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "Using cached pydantic_core-2.14.5-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Using cached torchvision-0.16.1-cp311-cp311-macosx_11_0_arm64.whl (1.5 MB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached google_auth-2.25.2-py2.py3-none-any.whl (184 kB)\n",
      "Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Using cached httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl (145 kB)\n",
      "Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Using cached protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Using cached uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "Using cached watchfiles-0.21.0-cp311-cp311-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "Using cached websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl (121 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: sentencepiece, pytz, pypika, monotonic, mmh3, flatbuffers, zipp, wrapt, websockets, websocket-client, uvloop, urllib3, tzdata, termcolor, tenacity, sniffio, python-dotenv, pypdf, pydantic-core, pyasn1, pulsar-client, protobuf, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, oauthlib, importlib-resources, humanfriendly, httptools, h11, grpcio, flaml, distro, diskcache, click, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, annotated-types, xgboost, uvicorn, typer, rsa, pydantic, pyasn1-modules, pandas, opentelemetry-proto, nltk, importlib-metadata, httpcore, googleapis-common-protos, deprecated, coloredlogs, anyio, watchfiles, torchvision, tiktoken, starlette, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, httpx, google-auth, opentelemetry-sdk, opentelemetry-instrumentation, openai, kubernetes, fastapi, pyautogen, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, sentence-transformers, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "Successfully installed annotated-types-0.6.0 anyio-3.7.1 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.1 cachetools-5.3.2 chroma-hnswlib-0.7.3 chromadb-0.4.18 click-8.1.7 coloredlogs-15.0.1 deprecated-1.2.14 diskcache-5.6.3 distro-1.8.0 fastapi-0.104.1 flaml-2.1.1 flatbuffers-23.5.26 google-auth-2.25.2 googleapis-common-protos-1.62.0 grpcio-1.60.0 h11-0.14.0 httpcore-1.0.2 httptools-0.6.1 httpx-0.25.2 humanfriendly-10.0 importlib-metadata-6.11.0 importlib-resources-6.1.1 kubernetes-28.1.0 mmh3-4.0.1 monotonic-1.6 nltk-3.8.1 oauthlib-3.2.2 onnxruntime-1.16.3 openai-1.3.8 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 pandas-2.1.4 posthog-3.1.0 protobuf-4.25.1 pulsar-client-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyautogen-0.2.2 pydantic-2.5.2 pydantic-core-2.14.5 pypdf-3.17.2 pypika-0.48.9 python-dotenv-1.0.0 pytz-2023.3.post1 requests-oauthlib-1.3.1 rsa-4.9 sentence-transformers-2.2.2 sentencepiece-0.1.99 sniffio-1.3.0 starlette-0.27.0 tenacity-8.2.3 termcolor-2.4.0 tiktoken-0.5.2 torchvision-0.16.1 typer-0.9.0 tzdata-2023.3 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websocket-client-1.7.0 websockets-12.0 wrapt-1.16.0 xgboost-2.0.2 zipp-3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"pyautogen[retrievechat]~=0.2.0b5\" \"flaml[automl]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The specified config_list file './OAI_CONFIG_LIST' does not exist.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautogen\u001b[39;00m\n\u001b[1;32m      3\u001b[0m config_list \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mconfig_list_from_json(\n\u001b[1;32m      4\u001b[0m     env_or_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOAI_CONFIG_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     file_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     },\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(config_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels to use: \u001b[39m\u001b[38;5;124m\"\u001b[39m, [config_list[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(config_list))])\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"gpt4\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\",\n",
    "            \"gpt-35-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 and gpt-3.5-turbo models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct agents for RetrieveChat\n",
    "\n",
    "We start by initialzing the `RetrieveAssistantAgent` and `RetrieveUserProxyAgent`. The system message needs to be set to \"You are a helpful assistant.\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `RetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a retrieval augmented generation task for an initial prompt to be sent to the LLM assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepted file formats for that can be stored in \n",
    "# a vector database instance\n",
    "from autogen.retrieve_utils import TEXT_FORMATS\n",
    "\n",
    "print(\"Accepted file formats for `docs_path`:\")\n",
    "print(TEXT_FORMATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "# By default, the human_input_mode is \"ALWAYS\", which means the agent will ask for human input at every step. We set it to \"NEVER\" here.\n",
    "# `docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default,\n",
    "# it is set to None, which works only if the collection is already created.\n",
    "# `task` indicates the kind of task we're working on. In this example, it's a `code` task.\n",
    "# `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\n",
    "# `custom_text_types` is a list of file types to be processed. Default is `autogen.retrieve_utils.TEXT_FORMATS`.\n",
    "# This only applies to files under the directories in `docs_path`. Explictly included files and urls will be chunked regardless of their types.\n",
    "# In this example, we set it to [\"mdx\"] to only process markdown files. Since no mdx files are included in the `websit/docs`,\n",
    "# no files there will be processed. However, the explicitly included urls will still be processed.\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            os.path.join(os.path.abspath(''), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": [\"mdx\"],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"get_or_create\": True,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False, # set to False if you don't want to execute the code\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-1\"></a>\n",
    "### Example 1\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to help generate sample code and automatically run the code and fix errors if there is any.\n",
    "\n",
    "Problem: Which API should I use if I want to use FLAML for a classification task and I want to train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# given a problem, we use the ragproxyagent to generate a prompt to be sent to the assistant as the initial message.\n",
    "# the assistant receives the message and generates a response. The response will be sent back to the ragproxyagent for processing.\n",
    "# The conversation continues until the termination condition is met, in RetrieveChat, the termination condition when no human-in-loop is no code block detected.\n",
    "# With human-in-loop, the conversation will continue until the user says \"exit\".\n",
    "code_problem = \"How can I use FLAML to perform a classification task and use spark to do parallel training. Train 30 seconds and force cancel jobs if time limit is reached.\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=code_problem, search_string=\"spark\")  # search_string is used as an extra filter for the embeddings search, in this case, we only want to search documents that contain \"spark\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-2\"></a>\n",
    "### Example 2\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to answer a question that is not related to code generation.\n",
    "\n",
    "Problem: Who is the author of FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "qa_problem = \"Who is the author of FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=qa_problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-3\"></a>\n",
    "### Example 3\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to help generate sample code and ask for human-in-loop feedbacks.\n",
    "\n",
    "Problem: how to build a time series forecasting model for stock price using FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# set `human_input_mode` to be `ALWAYS`, so the agent will ask for human input at every step.\n",
    "ragproxyagent.human_input_mode = \"ALWAYS\"\n",
    "code_problem = \"how to build a time series forecasting model for stock price using FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=code_problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-4\"></a>\n",
    "### Example 4\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to answer a question and ask for human-in-loop feedbacks.\n",
    "\n",
    "Problem: Is there a function named `tune_automl` in FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# set `human_input_mode` to be `ALWAYS`, so the agent will ask for human input at every step.\n",
    "ragproxyagent.human_input_mode = \"ALWAYS\"\n",
    "qa_problem = \"Is there a function named `tune_automl` in FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=qa_problem)  # type \"exit\" to exit the conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-5\"></a>\n",
    "### Example 5\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to answer questions for [NaturalQuestion](https://ai.google.com/research/NaturalQuestions) dataset.\n",
    "\n",
    "First, we will create a new document collection which includes all the contextual corpus. Then, we will choose some questions and utilize RetrieveChat to answer them. For this particular example, we will be using the `gpt-3.5-turbo` model, and we will demonstrate RetrieveChat's feature of automatically updating context in case the documents retrieved do not contain sufficient information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list[0][\"model\"] = \"gpt-35-turbo\"  # change model to gpt-35-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/corpus.txt\"\n",
    "\n",
    "# Create a new collection for NaturalQuestions dataset\n",
    "# `task` indicates the kind of task we're working on. In this example, it's a `qa` task.\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": corpus_file,\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"natural-questions\",\n",
    "        \"chunk_mode\": \"one_line\",\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# queries_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/queries.jsonl\"\n",
    "queries = \"\"\"{\"_id\": \"ce2342e1feb4e119cb273c05356b33309d38fa132a1cbeac2368a337e38419b8\", \"text\": \"what is non controlling interest on balance sheet\", \"metadata\": {\"answer\": [\"the portion of a subsidiary corporation 's stock that is not owned by the parent corporation\"]}}\n",
    "{\"_id\": \"3a10ff0e520530c0aa33b2c7e8d989d78a8cd5d699201fc4b13d3845010994ee\", \"text\": \"how many episodes are in chicago fire season 4\", \"metadata\": {\"answer\": [\"23\"]}}\n",
    "{\"_id\": \"fcdb6b11969d5d3b900806f52e3d435e615c333405a1ff8247183e8db6246040\", \"text\": \"what are bulls used for on a farm\", \"metadata\": {\"answer\": [\"breeding\", \"as work oxen\", \"slaughtered for meat\"]}}\n",
    "{\"_id\": \"26c3b53ec44533bbdeeccffa32e094cfea0cc2a78c9f6a6c7a008ada1ad0792e\", \"text\": \"has been honoured with the wisden leading cricketer in the world award for 2016\", \"metadata\": {\"answer\": [\"Virat Kohli\"]}}\n",
    "{\"_id\": \"0868d0964c719a52cbcfb116971b0152123dad908ac4e0a01bc138f16a907ab3\", \"text\": \"who carried the usa flag in opening ceremony\", \"metadata\": {\"answer\": [\"Erin Hamlin\"]}}\n",
    "\"\"\"\n",
    "queries = [json.loads(line) for line in queries.split(\"\\n\") if line]\n",
    "questions = [q[\"text\"] for q in queries]\n",
    "answers = [q[\"metadata\"][\"answer\"] for q in queries]\n",
    "print(questions)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(questions)):\n",
    "    print(f\"\\n\\n>>>>>>>>>>>>  Below are outputs of Case {i+1}  <<<<<<<<<<<<\\n\\n\")\n",
    "\n",
    "    # reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "    assistant.reset()\n",
    "    \n",
    "    qa_problem = questions[i]\n",
    "    ragproxyagent.initiate_chat(assistant, problem=qa_problem, n_results=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, questions were directly selected from the dataset. RetrieveChat was able to answer the questions correctly in the first attempt as the retrieved context contained the necessary information in the first two cases. However, in the last three cases, the context with the highest similarity to the question embedding did not contain the required information to answer the question. As a result, the LLM model responded with `UPDATE CONTEXT`. With the unique and innovative ability to update context in RetrieveChat, the agent automatically updated the context and sent it to the LLM model again. After several rounds of this process, the agent was able to generate the correct answer to the questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-6\"></a>\n",
    "### Example 6\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to answer multi-hop questions for [2WikiMultihopQA](https://github.com/Alab-NII/2wikimultihop) dataset with customized prompt and few-shot learning.\n",
    "\n",
    "First, we will create a new document collection which includes all the contextual corpus. Then, we will choose some questions and utilize RetrieveChat to answer them. For this particular example, we will be using the `gpt-3.5-turbo` model, and we will demonstrate RetrieveChat's feature of automatically updating context in case the documents retrieved do not contain sufficient information. Moreover, we'll demonstrate how to use customized prompt and few-shot learning to address tasks that are not pre-defined in RetrieveChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_MULTIHOP = \"\"\"You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the context provided by the user. You must think step-by-step.\n",
    "First, please learn the following examples of context and question pairs and their corresponding answers.\n",
    "\n",
    "Context:\n",
    "Kurram Garhi: Kurram Garhi is a small village located near the city of Bannu, which is the part of Khyber Pakhtunkhwa province of Pakistan. Its population is approximately 35000.\n",
    "Trojkrsti: Trojkrsti is a village in Municipality of Prilep, Republic of Macedonia.\n",
    "Q: Are both Kurram Garhi and Trojkrsti located in the same country?\n",
    "A: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia. Thus, they are not in the same country. So the answer is: no.\n",
    "\n",
    "\n",
    "Context:\n",
    "Early Side of Later: Early Side of Later is the third studio album by English singer- songwriter Matt Goss. It was released on 21 June 2004 by Concept Music and reached No. 78 on the UK Albums Chart.\n",
    "What's Inside: What's Inside is the fourteenth studio album by British singer- songwriter Joan Armatrading.\n",
    "Q: Which album was released earlier, What'S Inside or Cassandra'S Dream (Album)?\n",
    "A: What's Inside was released in the year 1995. Cassandra's Dream (album) was released in the year 2008. Thus, of the two, the album to release earlier is What's Inside. So the answer is: What's Inside.\n",
    "\n",
    "\n",
    "Context:\n",
    "Maria Alexandrovna (Marie of Hesse): Maria Alexandrovna , born Princess Marie of Hesse and by Rhine (8 August 1824  3 June 1880) was Empress of Russia as the first wife of Emperor Alexander II.\n",
    "Grand Duke Alexei Alexandrovich of Russia: Grand Duke Alexei Alexandrovich of Russia,(Russian:  ; 14 January 1850 (2 January O.S.) in St. Petersburg  14 November 1908 in Paris) was the fifth child and the fourth son of Alexander II of Russia and his first wife Maria Alexandrovna (Marie of Hesse).\n",
    "Q: What is the cause of death of Grand Duke Alexei Alexandrovich Of Russia's mother?\n",
    "A: The mother of Grand Duke Alexei Alexandrovich of Russia is Maria Alexandrovna. Maria Alexandrovna died from tuberculosis. So the answer is: tuberculosis.\n",
    "\n",
    "\n",
    "Context:\n",
    "Laughter in Hell: Laughter in Hell is a 1933 American Pre-Code drama film directed by Edward L. Cahn and starring Pat O'Brien. The film's title was typical of the sensationalistic titles of many Pre-Code films.\n",
    "Edward L. Cahn: Edward L. Cahn (February 12, 1899  August 25, 1963) was an American film director.\n",
    "Q: When did the director of film Laughter In Hell die?\n",
    "A: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the answer is: August 25, 1963.\n",
    "\n",
    "Second, please complete the answer by thinking step-by-step.\n",
    "\n",
    "Context:\n",
    "{input_context}\n",
    "Q: {input_question}\n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "corpus_file = \"https://huggingface.co/datasets/thinkall/2WikiMultihopQA/resolve/main/corpus.txt\"\n",
    "\n",
    "# Create a new collection for NaturalQuestions dataset\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": corpus_file,\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"2wikimultihopqa\",\n",
    "        \"chunk_mode\": \"one_line\",\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"customized_prompt\": PROMPT_MULTIHOP,\n",
    "        \"customized_answer_prefix\": \"the answer is\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# queries_file = \"https://huggingface.co/datasets/thinkall/2WikiMultihopQA/resolve/main/queries.jsonl\"\n",
    "queries = \"\"\"{\"_id\": \"61a46987092f11ebbdaeac1f6bf848b6\", \"text\": \"Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\", \"metadata\": {\"answer\": [\"The Mask Of Fu Manchu\"]}}\n",
    "{\"_id\": \"a7b9672009c311ebbdb0ac1f6bf848b6\", \"text\": \"Are North Marion High School (Oregon) and Seoul High School both located in the same country?\", \"metadata\": {\"answer\": [\"no\"]}}\n",
    "\"\"\"\n",
    "queries = [json.loads(line) for line in queries.split(\"\\n\") if line]\n",
    "questions = [q[\"text\"] for q in queries]\n",
    "answers = [q[\"metadata\"][\"answer\"] for q in queries]\n",
    "print(questions)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(questions)):\n",
    "    print(f\"\\n\\n>>>>>>>>>>>>  Below are outputs of Case {i+1}  <<<<<<<<<<<<\\n\\n\")\n",
    "\n",
    "    # reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "    assistant.reset()\n",
    "    \n",
    "    qa_problem = questions[i]\n",
    "    ragproxyagent.initiate_chat(assistant, problem=qa_problem, n_results=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
